---
layout: post
showcase: false
title: E#0 车牌识别 
img: chepaishibie.jpg
color: FF6F00
author: 扑兔
description: 这是第零号实验记录，训练集来自 CCPD，代码托管在 GitHub，使用 YOLOv3 识别车牌，“扭正”，再识别车牌内容。经过一番对比市面上的商用识别以及几个开源项目，发现，基本上蓝牌、黄牌都能正确识别，某些项目不能识别新能源车牌，我还没有见到能正确识别武警车牌、北京白底车牌，以及网上搜到的个性牌。
---

**前言**  
这是实验(E#0)记录，不是教程。  

**训练集**  
首先找到了 [CCPD](https://github.com/detectRecog/CCPD) 提供的训练集。  
压缩包里按照类别建立文件夹，比如普通的图片在 base 中，模糊的图片在 blur 中，  
倾斜的图片在 rotate 和 tilt 中。还有没车牌号的图片在 np 中。  
图片的文件名包含了图中的信息，首先用符号 - 分为7部分，第 3 项是车牌外框，左上及右下坐标；
第 4 项是车牌的 4 个顶点坐标，比外框更“贴身”，  
顺序为：右下，左下，左上，右上。但是标记的有点“飘逸”。  
训练集的数量还可以，但是比较偏，绝大多数都是“皖”部地区的牌子，几乎都是蓝牌。  
![LPSample](/images/ASL4182019-07-15-12-16-56.png)

**想法**  
由于数据不“齐全”，因此初步计划放弃 End-to-End 比较好。  

决定分两步，第一步识别车牌的位置，然后把各种方向歪曲的车牌图片“扭正”；第二步识别车牌内容。  
有了第一步以后，可以考虑爬虫抓取带车牌的图片，扩展训练集合。
第一步中，semantic segmentation 是否有帮助？  
Ng 老师曾建议要从最简单的模型开始，看看效果怎么样，分析问题在哪里，再改进。  

前段时间通过阅读 YOLOv3 的文献、解析以及 TensorFlow 和 PyTorch 的实现，目前了解了 YOLOv3 的基本思想，  
从其中可知，之前设想的直接预测车牌的“贴身坐标”效果不好，结果不稳定，当然这有待于实践验证。  
那么作为探索，还是从比较稳定的方法下手不容易有挫败感，于是决定就用 YOLOv3 检测。  
可是这样的话，就没法“扭正图片啦”。那么下一步怎么操作还没想好。  
数据扩增：
YOLOv3 接受 416×416 的图片，CCPD2019 的图片为 720×1160 大小。  
计划分为三个档：  

- 将图片缩小 1160/416 倍，两边补灰色；  
- 将图片缩小 720/416 倍，然后取上中下 3 块正方形区域；  
- 图片大小不做变化，直接到图片上随机取几块 416×416；  

实际操作时发现，YunYang1994 的开源实现中已经做了数据扩增：  
在 dataset.py 中包含了 3 种图片变换的方法，每种变换有 50% 的几率得到执行。作者的意思是多跑几圈，每次的数据都可能不太一样。  
我的想法是先将图片处理好，或者用 tf.data.Dataset 在运行时，进 GPU 之前把图片扩增。这两种方法有没有实际效果上的区别呢？  
我觉得 YOLOv3 天生就会分三种放大倍率来学习数据，那么放大取图片一部分的扩增方法可能作用就不大了。不过这样处理会产生车牌在图片的角落显示不全的样本，对学习可能有些帮助吧。  
我决定实操时先用 YunYang1994 现成的方案，然后再尝试“魔改”对比有没有效果。需要做的看来只是将图片变成 416×416，咋变不管。

通过 CCPD 中“四个定点”来生成数据，扭正后适当旋转，同一照片中包含数张车牌。  
用人造数据进行训练，观察实际效果咋样。  

**车牌位置**  

**识别车牌内容**  

**代码**  
[https://github.com/DamageControlStudio/LPSayWhat](https://github.com/DamageControlStudio/LPSayWhat)  

**思考**  

- 识别车体放大的牌号
- 部分遮挡的识别  
- 车标、车体颜色、车型（小汽车、卡车、货车）的识别

**实验记录**  
第一次尝试，350000 张 CCPD 训练集，留下 10000 张用于测试，其他全部喂进神经网络，运行一小时后突然想起来，这要是 30 个 epoch，粗略算下来得跑上一个月。但是我没有立即 ctrl+c，而是仔细看了一下 train.py 每个 epoch 运行下来是有保存的，所以，至少跑完一轮再看看。  
结果数个小时后，遇到了 loss 变成 nan，此时，1 个 epoch 还没完事，log 如下：  

``` cmdline
=> STEP 46129   lr: 0.000270   giou_loss: 0.56   conf_loss: 0.03   prob_loss: 0.00   total_loss: 0.59  
=> STEP 46130   lr: 0.000270   giou_loss: 0.39   conf_loss: 0.02   prob_loss: 0.00   total_loss: 0.41  
=> STEP 46131   lr: 0.000270   giou_loss: 0.37   conf_loss: 0.01   prob_loss: 0.00   total_loss: 0.38  
=> STEP 46132   lr: 0.000270   giou_loss:  nan   conf_loss: 0.88   prob_loss: 0.00   total_loss:  nan  
=> STEP 46133   lr: 0.000270   giou_loss:  nan   conf_loss:  nan   prob_loss:  nan   total_loss:  nan  
=> STEP 46134   lr: 0.000270   giou_loss:  nan   conf_loss:  nan   prob_loss:  nan   total_loss:  nan  
```

查阅 Google 发现大部分 nan 都是梯度炸了，YunYang1994 和 StackOverflow 上有人也提到可能存在错误的数据。  
From StackOverflow  
Good question.  
I came across this phenomenon several times. Here are my observations:  
Gradient blow up  
Reason: large gradients throw the learning process off-track.  
What you should expect: Looking at the runtime log, you should look at the loss values per-iteration. You'll notice that the loss starts to grow significantly from iteration to iteration, eventually the loss will be too large to be represented by a floating point variable and it will become nan.  
What can you do: Decrease the base_lr (in the solver.prototxt) by an order of magnitude (at least). If you have several loss layers, you should inspect the log to see which layer is responsible for the gradient blow up and decrease the loss_weight (in train_val.prototxt) for that specific layer, instead of the general base_lr.  
Bad learning rate policy and params  
Reason: caffe fails to compute a valid learning rate and gets 'inf' or 'nan' instead, this invalid rate multiplies all updates and thus invalidating all parameters.  
What you should expect: Looking at the runtime log, you should see that the learning rate itself becomes 'nan', for example:  
    ... sgd_solver.cpp:106] Iteration 0, lr = -nan  
What can you do: fix all parameters affecting the learning rate in your 'solver.prototxt' file.  
For instance, if you use lr_policy: "poly" and you forget to define max_iter parameter, you'll end up with lr = nan...  
For more information about learning rate in caffe, see this thread.  
Faulty Loss function  
Reason: Sometimes the computations of the loss in the loss layers causes nans to appear. For example, Feeding InfogainLoss layer with non-normalized values, using custom loss layer with bugs, etc.  
What you should expect: Looking at the runtime log you probably won't notice anything unusual: loss is decreasing gradually, and all of a sudden a nan appears.  
What can you do: See if you can reproduce the error, add printout to the loss layer and debug the error.  
For example: Once I used a loss that normalized the penalty by the frequency of label occurrence in a batch. It just so happened that if one of the training labels did not appear in the batch at all - the loss computed produced nans. In that case, working with large enough batches (with respect to the number of labels in the set) was enough to avoid this error.  
Faulty input  
Reason: you have an input with nan in it!  
What you should expect: once the learning process "hits" this faulty input - output becomes nan. Looking at the runtime log you probably won't notice anything unusual: loss is decreasing gradually, and all of a sudden a nan appears.  
What can you do: re-build your input datasets (lmdb/leveldn/hdf5...) make sure you do not have bad image files in your training/validation set. For debug you can build a simple net that read the input layer, has a dummy loss on top of it and runs through all the inputs: if one of them is faulty, this dummy net should also produce nan.  
stride larger than kernel size in "Pooling" layer  
For some reason, choosing stride > kernel_size for pooling may results with nans. For example:  

``` json
layer {
  name: "faulty_pooling"
  type: "Pooling"
  bottom: "x"
  top: "y"
  pooling_param {
    pool: AVE
    stride: 5
    kernel: 3
  }
}
```

results with nans in y.  
Instabilities in "BatchNorm"  
It was reported that under some settings "BatchNorm" layer may output nans due to numerical instabilities.
This issue was raised in bvlc/caffe and PR #5136 is attempting to fix it.  
Recently, I became aware of debug_info flag: setting debug_info: true in 'solver.prototxt' will make caffe print to log more debug information (including gradient magnitudes and activation values) during training: This information can help in spotting gradient blowups and other problems in the training process.  

然后发现，产生 nan 以后，模型虽然保存了，然是完全不能用了，预测每张图片都是空的。  
于是思索后来了第二次尝试：这次只从清晰的图片中取了一万张用于训练，暂时没改 lr。同时添加了提醒代码，当检测到 loss 那一行 log 包含 nan（ 或 inf）时，停止模型，保存模型，通知到手机（微信或者 pushover 等）。这段检测通知代码应该是需要放到 optimizer.apply_gradients() 前，要不然就更新参数了，是这样吧？  
这次跑了两三个小时，又出现了 nan，已经跑完了一遍 epoch，说明不是数据有问题，还是 lr 高了。  
第三次，把 lr 降了一个数量级，1e-4。  
这次完整地跑完了，损失函数也变得很小。经过检验，发现在 CCPD 数据集表现很好，然后从网上下载一些类似的照片，识别的也不错。  
不过用 iPhone 在街上找的照片识别起来就差很多，这也在意料之中，因为 iPhone 拍的照片分辨率很高，放大后看得清车牌，可是传入神经网络就被压缩成 416×416 的范围了，相当与变成了很小的目标。另外，新能源的绿色车牌与大型车辆的黄色车牌均不能识别，这也在意料之中。  
不过下面这张图片识别出的有些奇怪，车牌是黄底的（图片是 opencv 转换过的，所以 R 和 B 颜色通道是反的），可是车体的放大车牌号码被识别出来了（车身黄色），这是否说明：神经网络是通过汉字数字组合来识别车牌？  
![img1](/images/IMG_0339.JPG) ![img2](/images/IMG_0339_detect.JPG)  
无独有偶，[石星](https://cryer.github.io/2018/04/object_detection2/) 在做车牌检测任务时，贴出的图也有相似现象：  
![img3](/images/anotherplaterecg.jpg)  
他用的是印度的车牌数据库训练，车牌都是字母和数字。训练完，用中国的车牌子测试，发现框在了字母数字区域，而表示省份的打头汉字一般被无情地框在外面。所以，CNN 网络是通过学习认识了“一排字母和数字”。  

人在图片中识别文字，靠的是文字的线条与周围背景的对比度，是这样吗？那么是否应该诱导神经网络也这样学习 OCR 呢？

经过一番对比市面上的商用识别以及几个开源项目，发现，基本上蓝牌、黄牌都能正确识别，某些项目不能识别新能源车牌，我还没有见到能正确识别武警车牌、北京白底车牌，以及网上搜到的个性牌（中间包含“爱”字）。然鹅，这些车牌在人眼看来，不用想一定是车牌呀，只要是车牌，上面写啥都是车牌，上面印上国旗小标也是车牌，加上海边背景还是车牌。所以我觉得研究方向还应该再次转换，第一步不应局限于找车牌位置，应该找到图片所有的“牌”。在寻找可能存在的更广泛的数据集时发现了一篇论文，他的做法是先找到车，再定位牌，然后纠正角度与变形，最后 OCR 识别内容。这跟我的想法几乎是一样的，不过人家 2018 年已经实做到最后一步了。[论文地址](http://openaccess.thecvf.com/content_ECCV_2018/papers/Sergio_Silva_License_Plate_Detection_ECCV_2018_paper.pdf)  

数据来源的想法：
  
- 利用比较“初级”的车牌识别程序或者先用别人写好的识别程序，放出爬虫去找图片，然后再人工筛选、标注。缺点是工作量可能很大，另外存在偏差，比如爬虫不认识新能源绿色车牌，就爬不回来这样的测试集。  
- 利用 3D 建模的软件生成虚拟的各种场景。  
- 行车记录仪的数据人工标注。  
- 这里有个需要介绍信才能注册获得的[数据集](http://www.openits.cn/openData4/569.jhtml)  

2020-01-10:  
在实践中存在几个问题，因此花了一段时间构思了另外一个实验：骰子点数识别。具体需要解决的问题会在之后 E#1 中详细描述。  
因此车牌识别实验延后继续。  
